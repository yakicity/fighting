{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gymのインポート\n",
    "import gymnasium as gym\n",
    "# pandasのインポート\n",
    "import pandas as pd\n",
    "# matplotlibのインポート\n",
    "import matplotlib.pyplot  as plt\n",
    "import pygame\n",
    "import sys\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "# ニューラルネットワークモデルのインポート\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "# ACAgentクラスの作成\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def dist(a, b):\n",
    "    a1,a2 = a[0],a[1]\n",
    "    b1,b2 = b[0],b[1]\n",
    "    z_2 = (a2 - a1) ** 2 + (b2 - b1) ** 2\n",
    "    z = math.sqrt(z_2)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fighter import Fighter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACAgent(object):\n",
    "    def __init__(self, actions): # 初期化メソッド\n",
    "        self.actions = actions\n",
    "        self.model = None\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self): # モデルを初期化するメソッド\n",
    "        # モデルは中間層が1層で変数の数を32個\n",
    "        actor = MLPRegressor(hidden_layer_sizes=(32,), max_iter=1) # Actorのモデル\n",
    "        critic = MLPRegressor(hidden_layer_sizes=(32,), max_iter=1) # Criticのモデル\n",
    "        self.model = {'actor':actor, 'critic':critic} # ActorとCriticを辞書型として持っておく\n",
    "        self.initialized = True # 初期化フラグをTrueにしておく\n",
    "\n",
    "    def policy(self, state): # 状態を渡して行動を選択するメソッド\n",
    "        estimated = self.estimate(state)\n",
    "        prob_list = [np.exp(q)/np.exp(estimated).sum() for q in estimated]\n",
    "        prob_sum = sum(prob_list)\n",
    "        prob_list_normalized = [prob / prob_sum for prob in prob_list]\n",
    "        # print(sum(prob_list_normalized))\n",
    "        action = np.random.choice(self.actions, size = 1, p = prob_list_normalized)[0]\n",
    "        return action\n",
    "\n",
    "    def load_models(self):\n",
    "        models = {}\n",
    "        models['actor'] = joblib.load('actor.pkl') # actorのモデルを読み込む\n",
    "        models['critic'] = joblib.load('critic.pkl') # criticのモデルを読み込む\n",
    "        self.model = models\n",
    "        self.initialized = True # 初期化フラグをTrueにしておく\n",
    "\n",
    "    def update(self, experience, gamma): # 経験を蓄積したデータと割引率を渡して学習を行うメソッド\n",
    "        # 蓄積した経験において現在の状態と遷移先の状態の組を作る\n",
    "        states = [] # 現在の状態\n",
    "        new_states = [] # 遷移先の状態\n",
    "        for ex in experience:\n",
    "            states.append(ex['state'])\n",
    "            new_states.append(ex['new_state'])\n",
    "        states = np.concatenate(states, axis=0) # (n, 4)のnumpy.arrayとした\n",
    "        new_states = np.concatenate(new_states, axis=0) # (n, 4)のnumpy.arrayとした\n",
    "\n",
    "        # criticの学習\n",
    "        try: # partial_fitする前にpredictはできないため例外処理を実装する\n",
    "            estimated_values = self.model['critic'].predict(new_states) # 現在の状態に対する新しい価値評価の見積もり(n,)\n",
    "            for i, ex in enumerate(experience):\n",
    "                value = ex['reward']\n",
    "                if not ex['done']: # doneフラグがFalseの時(棒が倒れていない時)次の状態がある\n",
    "                    value += gamma*estimated_values[i]\n",
    "                estimated_values[i] = value\n",
    "        except NotFittedError:\n",
    "            estimated_values = np.random.random(size=len(states))\n",
    "        self.model['critic'].partial_fit(states, estimated_values) # 新しい価値の見積もりに近い出力になるように学習\n",
    "\n",
    "        # actorの学習\n",
    "        try: # partial_fitする前にpredictはできないため例外処理を実装する\n",
    "            estimated_action_values = self.model['actor'].predict(states) # 現在の状態に対する価値評価(n,len(self.actions))\n",
    "        except NotFittedError:\n",
    "            estimated_action_values = np.random.random(size=(len(states), len(self.actions)))\n",
    "        for i, ex in enumerate(experience): # とった行動に対して新しい価値評価の見積もりに変える\n",
    "            estimated_action_values[i, ex['action']] = estimated_values[i]\n",
    "        self.model['actor'].partial_fit(states, estimated_action_values) # 新しい価値の見積もりに近い出力になるように学習\n",
    "\n",
    "    def estimate(self, state): # 状態を渡して各行動の価値評価を推定するメソッド\n",
    "        if self.initialized:\n",
    "            return self.model['actor'].predict(state)[0] # (1,len(self.actions))の形で返るので、(len(self.actions),)で出力する\n",
    "        else:\n",
    "            return np.random.random(size=len(self.actions)) # 初期化フラグがFalseの時はランダムな値を出力する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "from pygame import gfxdraw\n",
    "import math\n",
    "import random\n",
    "from typing import Optional\n",
    "# reset(self) ：環境を初期状態にして初期状態(state)の観測(observation)をreturnする\n",
    "# step(self, action) ：行動を受け取り行動後の環境状態(state)の観測(observation)・即時報酬(reward)・エピソードの終了判定(done)・情報(info)をreturnする\n",
    "# render(self, mode) ：modeで指定されたように描画もしは配列をreturnする\n",
    "# close(self) ：環境を終了する際に必要なクリーンアップ処理を実施する\n",
    "# seed(self, seed=None) ：シードを設定する\n",
    "\n",
    "# 使用するデータ型\n",
    "# Discrete：[0, n-1]で指定したn個の離散値空間を扱う整数型（int）\n",
    "# 使い方はDiscrete(n)\n",
    "# Box：[low, high]で指定した連続値空間を扱う浮動小数点型（float）\n",
    "# 使い方はBox(low, high, shape, dtype)\n",
    "# lowおよびhighはshapeで与えたサイズと同じndarrayになります。\n",
    "# 次に、TupleとDictの使い方について示します。\n",
    "\n",
    "# Tuple：DiscreteやBoxなどの型をタプルで合体\n",
    "# 使い方の例はTuple((Discrete(2), Box(0, 1, (4, ))))\n",
    "# Dict：DiscreteやBoxなどの型を辞書で合体\n",
    "# 使い方の例はDict({'position':Discrete(2), 'velocity':Box(0, 1, (4, ))})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\"],\n",
    "        \"render_fps\": 20,\n",
    "    }\n",
    "\n",
    "    # def __init__(self,models,is_cpu,render_mode: Optional[str] = None):\n",
    "    def __init__(self,render_mode: Optional[str] = None):\n",
    "        # action_space ：エージェントが取りうる行動空間を定義\n",
    "        # observation_space：エージェントが受け取りうる観測空間を定義\n",
    "        # reward_range ：報酬の範囲[最小値と最大値]を定義\n",
    "\n",
    "        self.screen = None\n",
    "        # self.clock = None\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.window_x = 1000\n",
    "        self.window_y = 700\n",
    "\n",
    "        # self.models = models\n",
    "        # self.is_cpu = is_cpu\n",
    "        RIGIT_MAX = 20\n",
    "        self.jump_speed = 30  # <= ジャンプの初速度\n",
    "        self.gravity = 16\n",
    "        self.move_speed = 10\n",
    "        self.stage_pos = [0, 550]  # <= 表示するステージの位置\n",
    "        self.size = [50,80]\n",
    "        self.radius = 30\n",
    "\n",
    "        self.player1 = None\n",
    "        self.player2 = None\n",
    "\n",
    "        # アクション数定義\n",
    "        # 移動：「左」「右」「上」「移動なし」，攻撃：「する」「しない」\n",
    "        ACTION_NUM=8\n",
    "        self.action_space = gym.spaces.Discrete(ACTION_NUM)\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # 状態の範囲を定義,inattackrangeが１のときはどちらもアタックできる距離にある\n",
    "        # 水平距離，垂直距離，P1x,P1y,P2x,P2y,inatackrange,p1cooldown,p2cooldown,p1canjump,p2canjupm\n",
    "        max_distancex = self.window_x - self.size[0]\n",
    "        max_distancey = self.stage_pos[1] - self.size[1]\n",
    "        max_x = self.window_x - self.size[0]\n",
    "        max_y = self.stage_pos[1] - self.size[1]\n",
    "        LOW = np.array([0,0,0,0,0,0,0,0,0,0,0])\n",
    "        HIGH = np.array([max_distancex,max_distancey,max_x,max_y,max_x,max_y,1,RIGIT_MAX,RIGIT_MAX,1,1])\n",
    "        self.observation_space = gym.spaces.Box(low=LOW, high=HIGH)\n",
    "        # 即時報酬の値\n",
    "        self.reward_range = (-5,5)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # 環境を初期状態にする関数\n",
    "        # 初期状態をreturnする\n",
    "        # リセットの際に、乱数seedのリセットはしてはいけないので注意してください。\n",
    "        player1_pos = [600, 470]  # <= 操作キャラの位置\n",
    "        direction1 =3 #キャラの方向，0=上,1=した,2=右.3=左\n",
    "\n",
    "        player2_pos = [200, 470]\n",
    "        direction2 =2 #キャラの方向，0=上,1=した,2=右.3=左\n",
    "\n",
    "        self.player1 = Fighter(self.size, self.gravity, self.move_speed,self.jump_speed,player1_pos,direction1)\n",
    "        self.player2 = Fighter(self.size, self.gravity, self.move_speed,self.jump_speed,player2_pos,direction2)\n",
    "\n",
    "        #初期化\n",
    "        observation=[player1_pos[0]-player2_pos[0],player1_pos[1]-player2_pos[1],player1_pos[0],player1_pos[1],player2_pos[0],player2_pos[1],0,0,0,1,1]\n",
    "        return np.array(observation, dtype=np.float32), {}\n",
    "\n",
    "    def step(self, action_index):\n",
    "        # 行動を受け取り行動後の状態をreturnする\n",
    "        # stepメソッドは、action_spaceで定義された型に沿った行動値を受け取り、環境を1ステップだけ進めます。\n",
    "        # 進めた後の観測、報酬、終了判定、その他の情報を生成し、リターンします。\n",
    "        # infoにはデバックに役立つ情報などを辞書型として格納することができます。\n",
    "        # 唯一、自由に使える変数なので、存分にinfoを活用しましょう。\n",
    "\n",
    "        # observation ：object型。observation_spaceで設定した通りのサイズ・型のデータを格納。\n",
    "        # reward ：float型。reward_rangeで設定した範囲内の値を格納。\n",
    "        # done ：bool型。エピソードの終了判定。\n",
    "        # info ：dict型。デバッグに役立つ情報など自由に利用可能。\n",
    "\n",
    "        done=False\n",
    "\n",
    "        self.player1.controlfromAction(action_index)\n",
    "        self.player2.controlrandom()\n",
    "        # control_character_random(self.player2)\n",
    "\n",
    "        self.player1.contact_judgment(self.player2)\n",
    "        self.player2.contact_judgment(self.player1)\n",
    "\n",
    "        self.player1.move()\n",
    "        self.player2.move()\n",
    "\n",
    "        self.player1.contact_judgment(self.player2)\n",
    "        self.player2.contact_judgment(self.player1)\n",
    "\n",
    "        self.player1.character_action(self.player2)\n",
    "        self.player2.character_action(self.player1)\n",
    "\n",
    "        reward = 0\n",
    "        if any(self.player2.hit_judg):\n",
    "            reward += 1\n",
    "        if any(self.player1.hit_judg):\n",
    "            reward -= 1\n",
    "\n",
    "        if self.player1.damage >= 390:\n",
    "            reward = -5\n",
    "            done = True\n",
    "\n",
    "        if self.player2.damage >= 390:\n",
    "            reward = 5\n",
    "            done = True\n",
    "\n",
    "        self.player1.hit_action()\n",
    "        self.player2.hit_action()\n",
    "\n",
    "        self.player1.contact_judgment(self.player2)\n",
    "        self.player2.contact_judgment(self.player1)\n",
    "\n",
    "        inattackrange = 0\n",
    "        if self.player1.pos_x > self.player2.pos_x:\n",
    "            circle_pos = (self.player1.pos_x, self.player1.pos_y + self.player1.height // 2)\n",
    "            enemy_hit_pos = (self.player2.pos_x + self.player2.pos_x / 2, self.player2.pos_y + self.player2.height / 2)\n",
    "        else:\n",
    "            circle_pos = (self.player1.pos_x + self.player1.width, self.player1.pos_y + self.player1.height // 2)\n",
    "            enemy_hit_pos = (self.player2.pos_x, self.player2.pos_y + self.player2.height / 2)\n",
    "        print(dist(circle_pos,enemy_hit_pos))\n",
    "        if 0 <= dist(circle_pos,enemy_hit_pos) <= self.radius + self.player2.height / 2:\n",
    "            inattackrange = 1\n",
    "\n",
    "        p1canjump = 1 if self.player1.canMoveRange[0] == 0 else 0\n",
    "        p2canjump = 1 if self.player2.canMoveRange[0] == 0 else 0\n",
    "        # 水平距離，垂直距離，P1x,P1y,P2x,P2y,inattackrange,p1cooldown,p2cooldown,p1canjump,p2canjupm\n",
    "        observation=[abs(self.player1.pos_x - self.player2.pos_x),\n",
    "                     abs(self.player1.pos_y - self.player2.pos_y),\n",
    "                     self.player1.pos_x,\n",
    "                     self.player1.pos_y,\n",
    "                     self.player2.pos_x,\n",
    "                     self.player2.pos_y,\n",
    "                     inattackrange,\n",
    "                     self.player1.rigit_time,\n",
    "                     self.player2.rigit_time,\n",
    "                     p1canjump,\n",
    "                     p2canjump]\n",
    "\n",
    "        #ゲームが長いとマイナス\n",
    "        if done == False:\n",
    "            reward -= 0.1\n",
    "        # 今回の例ではtruncatedは使用しない\n",
    "        truncated = False\n",
    "        # 今回の例ではinfoは使用しない\n",
    "        info = {}\n",
    "        return np.array(observation, dtype=np.float32),reward,done,truncated,info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            return\n",
    "        if self.screen is None:\n",
    "            #初期化\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode((self.window_x, self.window_y)) # ウィンドウサイズの指定\n",
    "                # self.font = pygame.font.Font(None, 55)\n",
    "            else: # mode == \"rgb_array\"\n",
    "                self.screen = pygame.Surface((self.window_x, self.window_y))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        # modeとしてhuman, rgb_array, ansiが選択可能\n",
    "        # humanなら描画し, rgb_arrayならそれをreturnし, ansiなら文字列をreturnする\n",
    "\n",
    "        self.surf = pygame.Surface((self.window_x, self.window_y))\n",
    "        self.surf.fill((250, 250, 250))\n",
    "        # text = self.font.render('Score:'+str(self.point), True, (255,255,255))   # 描画する文字列の設定\n",
    "        # ステージの描画\n",
    "        pygame.draw.rect(self.surf , (0, 250, 0), (self.stage_pos[0], self.stage_pos[1], 1500, 50))\n",
    "\n",
    "\n",
    "        # プレイヤー1と2の描画(figureとaction.lifeの描画)\n",
    "        player1_rect = (self.player1.pos_x, self.player1.pos_y, self.size[0], self.size[1])\n",
    "        gfxdraw.rectangle(self.surf, player1_rect,(255,0,0))\n",
    "        player2_rect = (self.player2.pos_x, self.player2.pos_y, self.size[0], self.size[1])\n",
    "        gfxdraw.rectangle(self.surf, player2_rect,(255,0,0))\n",
    "\n",
    "        # 攻撃の描画\n",
    "        if self.player1.circle_pos is not None:\n",
    "            gfxdraw.filled_circle(self.surf,self.player1.circle_pos[0],self.player1.circle_pos[0],30,(0,0,250))\n",
    "        if self.player2.circle_pos is not None:\n",
    "            gfxdraw.filled_circle(self.surf,self.player2.circle_pos[0],self.player2.circle_pos[0],30,(0,0,250))\n",
    "\n",
    "        # lifeゲージの描画\n",
    "        gfxdraw.rectangle(self.surf, (570, 20, 400, 30), (120, 120, 120))\n",
    "        if self.player1.damage < 390:\n",
    "            gfxdraw.rectangle(self.surf, (575, 25, 390 - self.player1.damage, 20),(250, 200, 0))\n",
    "        gfxdraw.rectangle(self.surf, (30, 20, 400, 30), (120, 120, 120))\n",
    "        if self.player1.damage < 390:\n",
    "            gfxdraw.rectangle(self.surf, (35 + self.player2.damage, 25, 390 - self.player2.damage, 20),(250, 200, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, False)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        # self.screen.blit(text, [10, 10])# 文字列の表示位置\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "\n",
    "        # if self.render_mode == \"human\":\n",
    "        #     self.screen = pygame.display.set_mode((self.window_x, self.window_y)) # ウィンドウサイズの指定\n",
    "        #     # pygame.time.wait(30)#更新時間間隔\n",
    "        #     # pygame.display.set_caption(\"Pygame Test\") # ウィンドウの上の方に出てくるアレの指定\n",
    "        #     self.screen.fill((0,0,0,)) # 背景色の指定。RGBだと思う\n",
    "        #     text = self.font.render('Score:'+str(self.point), True, (255,255,255))   # 描画する文字列の設定\n",
    "        #     self.screen.blit(text, [10, 10])# 文字列の表示位置\n",
    "\n",
    "        #     pygame.draw.rect(self.screen, (255,0,0), (self.rect_x,self.rect_y,self.rect_width,self.rect_height))#的の描画\n",
    "        #     pygame.draw.circle(self.screen, (0,95,0), (self.ball_x,self.ball_y), 10, width=0)#ボールの描画\n",
    "        #     # pygame.draw.aaline(self.screen, (255,0,255), (self.ball_x,self.ball_y), (self.ball_x_next,self.ball_y_next), 0)#バーの描画\n",
    "        #     pygame.display.update() # 画面更新\n",
    "        #     pygame.event.pump()\n",
    "        #     self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        #     pygame.display.flip()\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n",
    "    def seed(self, seed=None):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observerクラスの作成\n",
    "class Observer(object):\n",
    "    def __init__(self, env): # 初期化メソッド\n",
    "        self.env = env\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def render(self): # 状態などを可視化するメソッド\n",
    "        self.env.render()\n",
    "\n",
    "    def reset(self): # 環境を初期化して初期状態を返すメソッド\n",
    "        return self.preprocess(self.env.reset()[0])\n",
    "\n",
    "    def step(self, action): # 行動を渡して前処理した状態と報酬などを返すメソッド\n",
    "        print(action)\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "        print(state)\n",
    "        return self.preprocess(state), reward, done, info\n",
    "    def preprocess(self, state):\n",
    "        return state.reshape((1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# models = [model, None]\n",
    "# is_cpu = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyEnv(render_mode=\"human\")\n",
    "observer = Observer(env) # Observer作成\n",
    "state = observer.reset() # observerを初期化し、前処理済みの初期状態を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "224.72205054244233\n",
      "[390.   0. 590. 470. 200. 470.   0.  19.   0.   1.   1.]\n",
      "\n",
      "行動: 1\n",
      "報酬: -0.1\n",
      "状態: [[390.   0. 590. 470. 200. 470.   0.  19.   0.   1.   1.]]\n",
      "None\n",
      "1\n",
      "235.63743335896356\n",
      "[390.   0. 580. 470. 190. 470.   0.  17.   0.   1.   1.]\n",
      "\n",
      "行動: 1\n",
      "報酬: -0.1\n",
      "状態: [[390.   0. 580. 470. 190. 470.   0.  17.   0.   1.   1.]]\n",
      "None\n",
      "1\n",
      "204.02205763103166\n",
      "[380.  30. 570. 470. 190. 440.   0.  15.   0.   1.   0.]\n",
      "\n",
      "行動: 1\n",
      "報酬: -0.1\n",
      "状態: [[380.  30. 570. 470. 190. 440.   0.  15.   0.   1.   0.]]\n",
      "None\n",
      "1\n",
      "202.27703774773843\n",
      "[360.  14. 560. 470. 200. 456.   0.  13.   0.   1.   0.]\n",
      "\n",
      "行動: 1\n",
      "報酬: -0.1\n",
      "状態: [[360.  14. 560. 470. 200. 456.   0.  13.   0.   1.   0.]]\n",
      "None\n",
      "1\n",
      "228.52789764052878\n",
      "[360.   0. 550. 470. 190. 470.   0.  11.  19.   1.   1.]\n",
      "\n",
      "行動: 1\n",
      "報酬: -0.1\n",
      "状態: [[360.   0. 550. 470. 190. 470.   0.  11.  19.   1.   1.]]\n",
      "None\n",
      "1\n",
      "241.8677324489565\n",
      "[360.   0. 540. 470. 180. 470.   0.   9.  17.   1.   1.]\n",
      "\n",
      "行動: 1\n",
      "報酬: -0.1\n",
      "状態: [[360.   0. 540. 470. 180. 470.   0.   9.  17.   1.   1.]]\n",
      "None\n",
      "1\n",
      "240.8318915758459\n",
      "[350.   0. 530. 470. 180. 470.   0.   7.  15.   1.   1.]\n",
      "\n",
      "行動: 1\n",
      "報酬: -0.1\n",
      "状態: [[350.   0. 530. 470. 180. 470.   0.   7.  15.   1.   1.]]\n",
      "None\n",
      "2\n",
      "214.11212016137713\n",
      "[360.  28. 540. 470. 180. 442.   0.   5.  13.   1.   0.]\n",
      "\n",
      "行動: 2\n",
      "報酬: -0.1\n",
      "状態: [[360.  28. 540. 470. 180. 442.   0.   5.  13.   1.   0.]]\n",
      "None\n",
      "2\n",
      "216.72332592501436\n",
      "[360.  12. 550. 470. 190. 458.   0.   3.  11.   1.   0.]\n",
      "\n",
      "行動: 2\n",
      "報酬: -0.1\n",
      "状態: [[360.  12. 550. 470. 190. 458.   0.   3.  11.   1.   0.]]\n",
      "None\n",
      "2\n",
      "245.15301344262525\n",
      "[380.   0. 560. 470. 180. 470.   0.   1.   9.   1.   1.]\n",
      "\n",
      "行動: 2\n",
      "報酬: -0.1\n",
      "状態: [[380.   0. 560. 470. 180. 470.   0.   1.   9.   1.   1.]]\n",
      "None\n",
      "2\n",
      "232.86262044390037\n",
      "[380.   0. 570. 470. 190. 470.   0.   0.   7.   1.   1.]\n",
      "\n",
      "行動: 2\n",
      "報酬: -0.1\n",
      "状態: [[380.   0. 570. 470. 190. 470.   0.   0.   7.   1.   1.]]\n",
      "None\n",
      "2\n",
      "235.63743335896356\n",
      "[390.   0. 580. 470. 190. 470.   0.  19.   5.   1.   1.]\n",
      "\n",
      "行動: 2\n",
      "報酬: -0.1\n",
      "状態: [[390.   0. 580. 470. 190. 470.   0.  19.   5.   1.   1.]]\n",
      "None\n",
      "2\n",
      "214.478437144623\n",
      "[400.  26. 590. 470. 190. 444.   0.  17.   3.   1.   0.]\n",
      "\n",
      "行動: 2\n",
      "報酬: -0.1\n",
      "状態: [[400.  26. 590. 470. 190. 444.   0.  17.   3.   1.   0.]]\n",
      "None\n",
      "2\n",
      "219.31712199461307\n",
      "[400.  10. 600. 470. 200. 460.   0.  15.   1.   1.   0.]\n",
      "\n",
      "行動: 2\n",
      "報酬: -0.1\n",
      "状態: [[400.  10. 600. 470. 200. 460.   0.  15.   1.   1.   0.]]\n",
      "None\n",
      "2\n",
      "232.59406699226014\n",
      "[410.   0. 610. 470. 200. 470.   0.  13.   0.   1.   1.]\n",
      "\n",
      "行動: 2\n",
      "報酬: -0.1\n",
      "状態: [[410.   0. 610. 470. 200. 470.   0.  13.   0.   1.   1.]]\n",
      "None\n",
      "2\n",
      "223.8861317723811\n",
      "[410.   0. 620. 470. 210. 470.   0.  11.  19.   1.   1.]\n",
      "\n",
      "行動: 2\n",
      "報酬: -0.1\n",
      "状態: [[410.   0. 620. 470. 210. 470.   0.  11.  19.   1.   1.]]\n",
      "None\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "actions = [1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2]\n",
    "for action in actions: # 定義した行動のリストを逐次的に入力していく\n",
    "    new_state, reward, done, info = observer.step(action) # 行動を入力して進める\n",
    "    print('\\n行動:', action)\n",
    "    print('報酬:', reward)\n",
    "    print('状態:', new_state)\n",
    "    print(observer.render()) # 状態の可視化\n",
    "    if done: # 終了判定(done)がTrueとなった場合終了\n",
    "        env.close()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observerクラスの作成\n",
    "class Observer(object):\n",
    "    def __init__(self, env): # 初期化メソッド\n",
    "        self.env = env\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def preprocess(self, state): # 状態に対する前処理メソッド\n",
    "        pass\n",
    "\n",
    "    def render(self): # 状態などを可視化するメソッド\n",
    "        self.env.render()\n",
    "\n",
    "    def reset(self): # 環境を初期化して初期状態を返すメソッド\n",
    "        return self.preprocess(self.env.reset()[0])\n",
    "\n",
    "    def step(self, action): # 行動を渡して前処理した状態と報酬などを返すメソッド\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "        return self.preprocess(state), reward, done, info\n",
    "\n",
    "\n",
    "# Agentクラスの作成\n",
    "# AgentはObserverから渡される状態と報酬によって適切な行動を行うモジュール\n",
    "class Agent(object):\n",
    "    def __init__(self, actions): # 初期化メソッド\n",
    "        self.actions = actions\n",
    "        self.model = None\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self): # モデルを初期化するメソッド\n",
    "        pass\n",
    "\n",
    "    def load_models(self): # 訓練済みモデルをロードする\n",
    "        pass\n",
    "\n",
    "    def estimate(self, state): # 状態を渡して各行動の価値評価を推定するメソッド\n",
    "        pass\n",
    "\n",
    "    def update(self, experience, gamma): # 経験を蓄積したデータと割引率を渡して学習を行うメソッド\n",
    "        pass\n",
    "\n",
    "    def policy(self, state): # 状態を渡して行動を選択するメソッド\n",
    "        estimated = self.estimate(state)\n",
    "        prob_list = [np.exp(q)/np.exp(estimated).sum() for q in estimated]\n",
    "        prob_sum = sum(prob_list)\n",
    "        prob_list_normalized = [prob / prob_sum for prob in prob_list]\n",
    "        # print(sum(prob_list_normalized))\n",
    "        action = np.random.choice(self.actions, size = 1, p = prob_list_normalized)[0]\n",
    "        return action\n",
    "\n",
    "# Trainerは、ObserverとAgentの間でやり取りを行い、蓄積した経験をAgentに学習させ、学習の進捗(得られた報酬の履歴)を管理するモジュール\n",
    "# Trainerクラスを作成\n",
    "class Trainer(object):\n",
    "    def __init__(self, num_episodes, gamma, buffer_length): # 初期化メソッド\n",
    "        self.num_episodes = num_episodes\n",
    "        self.gamma = gamma\n",
    "        self.buffer_length = buffer_length\n",
    "        self.experience = deque(maxlen=buffer_length)\n",
    "        self.rewards = []\n",
    "\n",
    "    def load(self, agent): # agentのモデルを読み込むメソッド\n",
    "        pass\n",
    "\n",
    "    def save(self, agent): # agentのモデルを保存するメソッド\n",
    "        pass\n",
    "\n",
    "    def begin_train(self, agent): # 学習の初めにモデルの初期化などを行うメソッド\n",
    "        pass\n",
    "\n",
    "    def batch_train(self, agent, batch_size): # 1ステップにおいてバッチサイズを渡してagentを学習するメソッド\n",
    "        pass\n",
    "\n",
    "    def train(self, agent, observer, batch_size): # agentとobserverとバッチサイズを渡して全体の学習を行うメソッド\n",
    "        training = False # 学習を行うフラグ\n",
    "        for episode in range(self.num_episodes):\n",
    "            s = observer.reset() # observerを初期化し、前処理済みの初期状態を返す\n",
    "            done = False # エピソードの終了フラグ\n",
    "            reward_per_episode = 0 # 1エピソード当たりの報酬の総和\n",
    "            while not done: # エピソードが終了しない間はずっと処理を行う\n",
    "                action = agent.policy(s) # agentが戦略に従って行動を選択する\n",
    "                new_state, reward, done, info = observer.step(action) # agentがとった行動に対してobserverが前処理済みの状態などを返す\n",
    "                ex = {'state':s, 'action':action, 'reward':reward, 'new_state':new_state, 'done':done} # 経験: {'state': 現在の状態, 'action':行動, 'reward':報酬, 'new_state':遷移先の状態, 'done':終了フラグ}\n",
    "                self.experience.append(ex) # appendメソッドで経験を蓄積\n",
    "                reward_per_episode += reward # 獲得報酬を計算\n",
    "                if not training: # 学習を行うフラグがFalseのとき\n",
    "                    self.begin_train(agent)\n",
    "                    training = True\n",
    "                else: # 学習を行うフラグがTrueのとき\n",
    "                    if len(self.experience) == self.buffer_length: # buffer_length分だけ経験が蓄積しているとき学習を行う\n",
    "                        self.batch_train(agent, batch_size) # agentを学習\n",
    "                s = new_state # 状態を更新\n",
    "            self.rewards.append(reward_per_episode) # appendメソッドで獲得した報酬を格納\n",
    "\n",
    "\n",
    "# CartPoleObserverクラスの作成\n",
    "class MyFight(Observer):\n",
    "    def preprocess(self, state):\n",
    "        return state.reshape((1, 40))\n",
    "\n",
    "class ACAgent(Agent):\n",
    "    def initialize(self): # モデルを初期化するメソッド\n",
    "        # モデルは中間層が1層で変数の数を32個\n",
    "        actor = MLPRegressor(hidden_layer_sizes=(32,), max_iter=1) # Actorのモデル\n",
    "        critic = MLPRegressor(hidden_layer_sizes=(32,), max_iter=1) # Criticのモデル\n",
    "        self.model = {'actor':actor, 'critic':critic} # ActorとCriticを辞書型として持っておく\n",
    "        self.initialized = True # 初期化フラグをTrueにしておく\n",
    "\n",
    "    def load_models(self):\n",
    "        models = {}\n",
    "        models['actor'] = joblib.load('actor.pkl') # actorのモデルを読み込む\n",
    "        models['critic'] = joblib.load('critic.pkl') # criticのモデルを読み込む\n",
    "        self.model = models\n",
    "        self.initialized = True # 初期化フラグをTrueにしておく\n",
    "\n",
    "    def update(self, experience, gamma): # 経験を蓄積したデータと割引率を渡して学習を行うメソッド\n",
    "        # 蓄積した経験において現在の状態と遷移先の状態の組を作る\n",
    "        states = [] # 現在の状態\n",
    "        new_states = [] # 遷移先の状態\n",
    "        for ex in experience:\n",
    "            states.append(ex['state'])\n",
    "            new_states.append(ex['new_state'])\n",
    "        states = np.concatenate(states, axis=0) # (n, 4)のnumpy.arrayとした\n",
    "        new_states = np.concatenate(new_states, axis=0) # (n, 4)のnumpy.arrayとした\n",
    "\n",
    "        # criticの学習\n",
    "        try: # partial_fitする前にpredictはできないため例外処理を実装する\n",
    "            estimated_values = self.model['critic'].predict(new_states) # 現在の状態に対する新しい価値評価の見積もり(n,)\n",
    "            for i, ex in enumerate(experience):\n",
    "                value = ex['reward']\n",
    "                if not ex['done']: # doneフラグがFalseの時(棒が倒れていない時)次の状態がある\n",
    "                    value += gamma*estimated_values[i]\n",
    "                estimated_values[i] = value\n",
    "        except NotFittedError:\n",
    "            estimated_values = np.random.random(size=len(states))\n",
    "        self.model['critic'].partial_fit(states, estimated_values) # 新しい価値の見積もりに近い出力になるように学習\n",
    "\n",
    "        # actorの学習\n",
    "        try: # partial_fitする前にpredictはできないため例外処理を実装する\n",
    "            estimated_action_values = self.model['actor'].predict(states) # 現在の状態に対する価値評価(n,len(self.actions))\n",
    "        except NotFittedError:\n",
    "            estimated_action_values = np.random.random(size=(len(states), len(self.actions)))\n",
    "        for i, ex in enumerate(experience): # とった行動に対して新しい価値評価の見積もりに変える\n",
    "            estimated_action_values[i, ex['action']] = estimated_values[i]\n",
    "        self.model['actor'].partial_fit(states, estimated_action_values) # 新しい価値の見積もりに近い出力になるように学習\n",
    "\n",
    "    def estimate(self, state): # 状態を渡して各行動の価値評価を推定するメソッド\n",
    "        if self.initialized:\n",
    "            return self.model['actor'].predict(state)[0] # (1,len(self.actions))の形で返るので、(len(self.actions),)で出力する\n",
    "        else:\n",
    "            return np.random.random(size=len(self.actions)) # 初期化フラグがFalseの時はランダムな値を出力する\n",
    "\n",
    "# CartPoleTrainerクラスの作成\n",
    "class CartPoleTrainer(Trainer):\n",
    "    def load(self, agent): # agentのモデルを読み込むメソッド\n",
    "        models = {}\n",
    "        models['actor'] = joblib.load('actor.pkl') # actorのモデルを読み込む\n",
    "        models['critic'] = joblib.load('critic.pkl') # criticのモデルを読み込む\n",
    "        agent.model = models\n",
    "        agent.initialized = True\n",
    "\n",
    "    def save(self, agent):\n",
    "        joblib.dump(agent.model['actor'], 'actor.pkl') # actorのモデルを'actor.pkl'に保存する\n",
    "        joblib.dump(agent.model['critic'], 'critic.pkl') # criticのモデルを'critic.pkl'に保存する\n",
    "\n",
    "    def begin_train(self, agent): # 学習の初めにモデルの初期化などを行うメソッド\n",
    "        agent.initialize() # モデルを初期化\n",
    "        batch = np.random.choice(self.experience, size=1, replace=False) # 適当にサンプリング\n",
    "        agent.update(batch, self.gamma) # 適当なデータで学習\n",
    "\n",
    "    def batch_train(self, agent, batch_size): # 1ステップにおいてバッチサイズを渡してagentを学習するメソッド\n",
    "        batch = np.random.choice(self.experience, size=batch_size, replace=False) # batch_size数分サンプリング\n",
    "        agent.update(batch, self.gamma) # サンプリングされたデータで学習\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CartPole-v0の攻略\n",
    "    # cart_pole_env = gym.make('CartPole-v0',render_mode='human')  # 環境作成\n",
    "    cart_pole_env = gym.make('CartPole-v1')  # 環境作成\n",
    "    cart_pole_observer = CartPoleObserver(cart_pole_env) # Observer作成\n",
    "    ac_agent = ACAgent(actions = [0, 1]) # Agent作成\n",
    "    cart_pole_trainer = CartPoleTrainer(num_episodes = 500, gamma = 0.9, buffer_length = 512) # Trainer作成\n",
    "    cart_pole_trainer.train(agent = ac_agent, observer = cart_pole_observer, batch_size = 128) # 学習を実行\n",
    "    cart_pole_trainer.save(ac_agent) # 学習済みのモデルを保存\n",
    "\n",
    "    # 学習曲線の描画\n",
    "    import matplotlib.pyplot as plt # matplotlib.pyplotのインポート\n",
    "\n",
    "    plt.plot(cart_pole_trainer.rewards) # 報酬の折れ線グラフの描画\n",
    "    plt.title('Train Curve', fontsize=20) # タイトルを設定\n",
    "    plt.ylabel('Rewards', fontsize=20) # 縦軸のラベルを設定\n",
    "    plt.xlabel('Episode', fontsize=20) # 横軸のラベルを指定\n",
    "    plt.show() # グラフを表示\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
